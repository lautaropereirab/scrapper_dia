{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (5.2.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (4.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from selenium) (4.12.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from selenium) (2.2.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (1.2.1)\n",
      "Requirement already satisfied: idna in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: unidecode in c:\\users\\phoen\\anaconda3\\envs\\data_science\\lib\\site-packages (1.3.8)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "     -------------------------------------- 147.9/147.9 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install lxml\n",
    "!pip install selenium\n",
    "!pip install unidecode\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from unidecode import unidecode\n",
    "import requests\n",
    "import lxml as lxml\n",
    "import html as html\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['almacen',\n",
       " 'bebidas',\n",
       " 'frescos',\n",
       " 'desayuno',\n",
       " 'limpieza',\n",
       " 'perfumeria',\n",
       " 'congelados',\n",
       " 'bebes-y-ninos',\n",
       " 'hogar-y-deco',\n",
       " 'mascotas',\n",
       " 'kiosco',\n",
       " 'frutas-y-verduras',\n",
       " 'electro-hogar']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa el driver de Selenium (asegúrate de tener el driver correspondiente instalado)\n",
    "driver = webdriver.Chrome()  # Puedes cambiar a otro navegador y/o ruta del driver si lo necesitas\n",
    "driver.maximize_window()\n",
    "\n",
    "#Link a la pagina web\n",
    "link = 'https://diaonline.supermercadosdia.com.ar/'\n",
    "link_sin_barra = 'https://diaonline.supermercadosdia.com.ar'\n",
    "\n",
    "#Abre la página web en el navegador controlado por Selenium\n",
    "driver.get(link)\n",
    "\n",
    "\n",
    "#Hacemos click en el boton de categorias\n",
    "button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'diaio-custom-mega-menu-0-x-custom-mega-menu-trigger__button')))\n",
    "button.click()\n",
    "#time.sleep(2)\n",
    "\n",
    "# Obtiene el HTML después de cargar todos los elementos\n",
    "html = driver.page_source\n",
    "\n",
    "# Cierra el navegador controlado por Selenium\n",
    "driver.quit()\n",
    "\n",
    "# Parsea el HTML con BeautifulSoup\n",
    "soup_1 = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "#Buscamos la lista de las categorias \n",
    "containers = soup_1.find_all(class_ = \"diaio-custom-mega-menu-0-x-custom-mega-menu-desktop__list-item\") #Aca filtramos por el elemento UL que contiene las categorias\n",
    "\n",
    "# Lista para almacenar los elementos filtrados\n",
    "categorias = []\n",
    "\n",
    "# Itera sobre los contenedores para filtrar los que contienen un div interno con la clase \"diaio-store-5-x-menuItem\"\n",
    "for container in containers:\n",
    "    c = container.find_all('a', class_='diaio-custom-mega-menu-0-x-category-list-item__container')\n",
    "    for i in c:\n",
    "        # Obtiene el texto después de </span>\n",
    "        categoria = i.find('span', class_='diaio-custom-mega-menu-0-x-category-list-item__icon-container').next_sibling.strip()\n",
    "        texto_procesado = unidecode(categoria).lower()\n",
    "        texto_url = texto_procesado.replace(' ','-')\n",
    "        categorias.append(texto_url)\n",
    "\n",
    "categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primer categoria\n",
    "driver = webdriver.Chrome()  # Puedes cambiar a otro navegador y/o ruta del driver si lo necesitas\n",
    "driver.maximize_window()\n",
    "primer_url = link + categorias[0]\n",
    "\n",
    "\n",
    "# Abre la página web en el navegador controlado por Selenium\n",
    "driver.get(primer_url)\n",
    "button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"vtex-search-result-3-x-seeMoreButton\")))\n",
    "time.sleep(3)\n",
    "driver.execute_script(\"arguments[0].click();\", button)\n",
    "time.sleep(3)\n",
    "\n",
    "# Obtiene el HTML después de cargar todos los elementos\n",
    "html_2 = driver.page_source\n",
    "\n",
    "# Cierra el navegador controlado por Selenium\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aceites-blend',\n",
       " 'aceites-de-girasol',\n",
       " 'aceites-de-oliva',\n",
       " 'aceites-de-soja',\n",
       " 'aceitunas-y-encurtidos',\n",
       " 'acetos-vinagres-y-limon',\n",
       " 'alfajores',\n",
       " 'arroz-doble',\n",
       " 'arroz-integral',\n",
       " 'arroz-largo',\n",
       " 'arroz-parboil',\n",
       " 'arroz-preparado',\n",
       " 'bizcochuelos',\n",
       " 'bolsas-saborizadoras',\n",
       " 'budines-y-magdalenas',\n",
       " 'caldos',\n",
       " 'caramelos-y-gomitas',\n",
       " 'chicles-y-chupetines',\n",
       " 'chocolates',\n",
       " 'coberturas',\n",
       " 'comidas-elaboradas',\n",
       " 'conservas-de-frutas',\n",
       " 'conservas-de-pescados',\n",
       " 'conservas-de-vegetales',\n",
       " 'especias',\n",
       " 'facturas-y-medialunas',\n",
       " 'fideos-largos',\n",
       " 'fideos-para-guiso',\n",
       " 'fideos-para-sopa',\n",
       " 'gelatinas',\n",
       " 'grisines-y-tostadas',\n",
       " 'harinas-de-maiz',\n",
       " 'harinas-de-trigo',\n",
       " 'ketchup',\n",
       " 'legumbres-y-semillas',\n",
       " 'mayonesa',\n",
       " 'mostaza',\n",
       " 'otras-especialidades',\n",
       " 'otros-aderezos',\n",
       " 'pan-de-hamburguesa-y-pancho',\n",
       " 'pan-de-molde',\n",
       " 'pan-dulce',\n",
       " 'pan-rallado-y-rebozadores',\n",
       " 'panes',\n",
       " 'papas-fritas',\n",
       " 'pastas-rellenas',\n",
       " 'polvo-para-hornear-y-esencias',\n",
       " 'postres-para-preparar',\n",
       " 'premezclas-dulces',\n",
       " 'premezclas-saladas',\n",
       " 'pure',\n",
       " 'sal',\n",
       " 'salsa-golf',\n",
       " 'semola',\n",
       " 'snacks',\n",
       " 'sopas',\n",
       " 'tomates-y-salsas',\n",
       " 'turrones-obleas-y-confitados']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsea el HTML con BeautifulSoup\n",
    "soup_2 = BeautifulSoup(html_2, 'lxml')\n",
    "containers = soup_2.find_all('div', class_='vtex-search-result-3-x-filter__container')\n",
    "containers\n",
    "\n",
    "\n",
    "#creamos el filtro que nos devuelve las sub-categorias\n",
    "filtros = []\n",
    "sub_categorias_sin_procesar = []\n",
    "sub_categorias = []\n",
    "\n",
    "for i in range(len(containers)):\n",
    "    value = containers[i].get_text()\n",
    "    filtros.append(value)\n",
    "    test = re.findall('Sub-Categoría', filtros[i])\n",
    "    if test:\n",
    "        texto_lista = containers[i].get_text(separator='\\n', strip=True).split('\\n')\n",
    "        sub_categorias_sin_procesar.extend(texto_lista[1:-1]) #Exluimos el ultimo registro\n",
    "\n",
    "#limpiamos el texto para poder generar las variables que necesitamos\n",
    "for i in sub_categorias_sin_procesar:\n",
    "    texto_procesado = unidecode(i).lower()\n",
    "    texto = texto_procesado.replace(' ','-')\n",
    "    texto_2 = texto.replace(',', '')\n",
    "    sub_categorias.append(texto_2)\n",
    "sub_categorias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primer sub-categoria\n",
    "\n",
    "#volvemos a entrar a la misma categoria\n",
    "driver = webdriver.Chrome()  # Puedes cambiar a otro navegador y/o ruta del driver si lo necesitas\n",
    "driver.maximize_window()\n",
    "primer_url = link + categorias[0]\n",
    "driver.get(primer_url)\n",
    "\n",
    "#Ahora generamos la categoria que queremos seleccionar\n",
    "button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"vtex-search-result-3-x-seeMoreButton\")))#Expandimos todas las secciones\n",
    "button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "selector_sub_categoria = f'label[for=\"category-3-{sub_categorias[18]}\"]' #Creamos la categoria que necesitamos para hacer click\n",
    "checkbox = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector_sub_categoria)))\n",
    "checkbox.click()\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()  # Puedes cambiar a otro navegador y/o ruta del driver si lo necesitas\n",
    "driver.maximize_window()\n",
    "primer_url = link + categorias[0]\n",
    "driver.get(primer_url)\n",
    "\n",
    "# Ahora generamos la categoria que queremos seleccionar\n",
    "button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"vtex-search-result-3-x-seeMoreButton\")))  # Expandimos todas las secciones\n",
    "button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "selector_sub_categoria = f'label[for=\"category-3-{sub_categorias[18]}\"]'  # Creamos la categoria que necesitamos para hacer click\n",
    "checkbox = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector_sub_categoria)))\n",
    "checkbox.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Esta parte va a scrollear hasta abajo y actualizar los objetos que componen la lista a obtener\n",
    "prev_elements_count = len(driver.find_elements(By.CLASS_NAME, 'vtex-search-result-3-x-galleryItem'))\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "    curr_elements_count = len(driver.find_elements(By.CLASS_NAME, 'vtex-search-result-3-x-galleryItem'))\n",
    "\n",
    "    if curr_elements_count == prev_elements_count:\n",
    "        break\n",
    "\n",
    "    # Actualizar el contador de elementos\n",
    "    prev_elements_count = curr_elements_count\n",
    "\n",
    "print(prev_elements_count)\n",
    "# Obtiene el HTML después de cargar todos los elementos\n",
    "html_3 = driver.page_source\n",
    "time.sleep(2)\n",
    "\n",
    "# Cierra el navegador controlado por Selenium\n",
    "driver.quit()\n",
    "\n",
    "soup_3 = BeautifulSoup(html_3, 'lxml')\n",
    "productos_categoria = soup_3.find_all('div', class_='vtex-search-result-3-x-galleryItem')\n",
    "print(len(productos_categoria))\n",
    "\n",
    "total_items = []\n",
    "\n",
    "for litem_listado in range(len(productos_categoria)):\n",
    "    try:\n",
    "        # Extraemos link de la imagen del producto\n",
    "        imagen = productos_categoria[litem_listado].find('img')\n",
    "        link_imagen = imagen['src'] if imagen else None\n",
    "\n",
    "        # Extraemos link a la web del producto en el supermercado\n",
    "        web_producto = productos_categoria[litem_listado].find('a')\n",
    "        url_producto = link_sin_barra + web_producto['href'] if web_producto else None\n",
    "\n",
    "        # Datos del producto\n",
    "        informacion_producto_wrapped = productos_categoria[litem_listado].find('div', 'vtex-flex-layout-0-x-flexCol')\n",
    "        informacion_producto = informacion_producto_wrapped.find_all('div', 'vtex-flex-layout-0-x-flexColChild') if informacion_producto_wrapped else []\n",
    "\n",
    "        # Creamos las listas soporte\n",
    "        datos_productos_sin_procesar = []\n",
    "        datos_productos_procesados = []\n",
    "        datos_productos_procesados.append(categorias[0])  # Agregamos la categoria a la que pertenece el producto\n",
    "        datos_productos_procesados.append(sub_categorias[18])  # Agregamos la categoria a la que pertenece el producto\n",
    "\n",
    "        # Con este bucle for extraemos la informacion sin procesar\n",
    "        for info in informacion_producto:\n",
    "            valor_lista = info.get_text(strip=True).replace('\\xa0', ' ')\n",
    "            datos_productos_sin_procesar.append(valor_lista)\n",
    "\n",
    "        # Generamos la expresion regular que extrae el precio correcto\n",
    "        # y conformamos la lista con los datos del producto\n",
    "        for i in range(0, 4):\n",
    "            datos_productos_procesados.append(datos_productos_sin_procesar[i] if i < len(datos_productos_sin_procesar) else None)\n",
    "\n",
    "        for i in range(len(datos_productos_sin_procesar)):\n",
    "            patron_regex_precios = r\"(\\$\\d{1,3}(?:\\.\\d{3})*,\\d{2})\"\n",
    "            resultado_regex = re.findall(patron_regex_precios, datos_productos_sin_procesar[i])\n",
    "            if resultado_regex:\n",
    "                datos_productos_procesados.extend(resultado_regex)\n",
    "\n",
    "        if len(datos_productos_procesados) == 7:\n",
    "            datos_productos_procesados.append(None)\n",
    "            datos_productos_procesados.append(datos_productos_sin_procesar[-1])\n",
    "        else:\n",
    "            datos_productos_procesados.append(datos_productos_sin_procesar[-1])\n",
    "\n",
    "        datos_productos_procesados.append(url_producto)  # Agregamos el link al producto\n",
    "        datos_productos_procesados.append(link_imagen)  # Agregamos el link a la imagen\n",
    "\n",
    "        total_items.append(datos_productos_procesados)\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el producto {litem_listado}: {e}\")\n",
    "\n",
    "nombre_dataframe = f'data_producto_{categorias[0]}_{sub_categorias[18]}'\n",
    "lista_columnas = ('categoria', 'producto', 'banner_producto', 'marca', 'descripcion_producto',\n",
    "                  'promocion', 'precio', 'precio_sin_descuento', 'precio_por_kilo',\n",
    "                  'url_producto', 'url_imagen')\n",
    "\n",
    "globals()[nombre_dataframe] = pd.DataFrame(total_items, columns=lista_columnas)\n",
    "ruta_csv = f'C:/Users/phoen/OneDrive/Escritorio/Lauta/Data Science/Projects/scrapper_dia/Productos obtenidos/{nombre_dataframe}.csv'\n",
    "globals()[nombre_dataframe].to_csv(ruta_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
